{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Schedule Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Spark environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%local\n",
    "import os\n",
    "import json\n",
    "\n",
    "username = os.environ['JUPYTERHUB_USER']\n",
    "\n",
    "configuration = dict(\n",
    "    name = \"%s-final-schedule\" % username,\n",
    "    executorMemory = \"4G\",\n",
    "    executorCores = 4,\n",
    "    numExecutors = 5,\n",
    "    driverMemory = \"4G\",\n",
    "    conf = {\n",
    "        # \"spark.pyspark.python\": \"/opt/anaconda3/bin/python3\", # Use python3\n",
    "        \"spark.jars.repositories\": \"https://repos.spark-packages.org\",\n",
    "        \"spark.jars.packages\": \"graphframes:graphframes:0.7.0-spark2.3-s_2.11\"\n",
    "    }\n",
    ")\n",
    "\n",
    "# set the application name as \"<your_gaspar_id>-homework3\"\n",
    "get_ipython().run_cell_magic('configure', line=\"-f\", cell=json.dumps(configuration))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%send_to_spark -i username -t str -n username"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('We are using Spark %s' % spark.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.addPyFile('graphframes_graphframes-0.7.0-spark2.3-s_2.11.jar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from graphframes import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data into spark dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stops = spark.read.orc('/data/sbb/orc/geostops')\n",
    "stop_times = spark.read.csv(\"/data/sbb/csv/timetable/stop_times/2019/05/07/stop_times.csv\", header=True).drop('pickup_type', 'drop_off_type')\n",
    "routes = spark.read.csv('/data/sbb/csv/timetable/routes/2019/05/07/routes.csv', header=True )\n",
    "calendar = spark.read.csv('/data/sbb/csv/timetable/calendar/2019/05/07/calendar.csv', header=True).drop('start_date','end_date')\n",
    "trips = spark.read.csv('/data/sbb/csv/timetable/trips/2019/05/07/trips.csv', header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter out stops out of the 15km radius from ZÃ¼rich HB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.functions import acos, asin, cos, sin, lit, toRadians, sqrt\n",
    "\n",
    "def haversine(theta):\n",
    "    return (lit(1) - cos(theta)) / lit(2)\n",
    "\n",
    "def haversine_dist(latitude_x, longitude_x, latitude_y, longitude_y):\n",
    "    latitude_x, longitude_x, latitude_y, longitude_y = toRadians(latitude_x), toRadians(longitude_x),\\\n",
    "                                                       toRadians(latitude_y), toRadians(longitude_y)\n",
    "    h = haversine(latitude_x - latitude_y) + cos(latitude_x) * cos(latitude_y) * haversine(longitude_x - longitude_y)\n",
    "    earth_radius = 6371.0\n",
    "    return acos(lit(1) - lit(2) * h) * earth_radius"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leave only stops in 15 km radius\n",
    "zurich_HB_lat, zurich_HB_lon = 47.378177, 8.540192\n",
    "stops = stops.withColumn('distance_zurich_HB', haversine_dist(lit(zurich_HB_lat), lit(zurich_HB_lon), stops.stop_lat, stops.stop_lon))\n",
    "stops = stops.filter(stops.distance_zurich_HB <= 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replace child stations by their parent station"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stops = stops.withColumn('true_stop_id', F.when(F.col('parent_station') != '', F.col('parent_station')).otherwise(F.col('stop_id')))\n",
    "stops = stops.withColumn('true_stop_id', F.col('true_stop_id')[0:7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_info = stops.join(stop_times.join(trips.join(calendar, on='service_id'), on='trip_id'), on='stop_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_info = all_info.drop('stop_id')\n",
    "all_info = all_info.withColumnRenamed('true_stop_id', 'stop_id').drop('true_stop_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stops = stops.drop('stop_id')\n",
    "stops = stops.withColumnRenamed('true_stop_id', 'stop_id').drop('true_stop_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute edge lists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distances between stops and walking edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stops_start = stops.withColumnRenamed('stop_id', 'start_vertex')\\\n",
    "                   .withColumnRenamed('stop_lat', 'stop_lat_start')\\\n",
    "                   .withColumnRenamed('stop_lon', 'stop_lon_start')\\\n",
    "              \n",
    "stops_end = stops.withColumnRenamed('stop_id', 'end_vertex')\\\n",
    "                 .withColumnRenamed('stop_lat', 'stop_lat_end')\\\n",
    "                 .withColumnRenamed('stop_lon', 'stop_lon_end')\\\n",
    "              \n",
    "\n",
    "all_distances = stops_start.crossJoin(stops_end).withColumn('distance', haversine_dist(F.col('stop_lat_start'), F.col('stop_lon_start'),\n",
    "                                                                                       F.col('stop_lat_end'), F.col('stop_lon_end')))\n",
    "\n",
    "walking_speed = 0.05\n",
    "walking_edges = all_distances.filter((F.col('distance') <= 0.5) & (F.col('start_vertex') != F.col('end_vertex')))\\\n",
    "               .withColumn('duration', F.col('distance') / walking_speed)\\\n",
    "               .select('start_vertex', 'end_vertex', 'duration')\n",
    "\n",
    "walking_edges = walking_edges.groupBy('start_vertex', 'end_vertex').agg({'duration' : 'avg'})\\\n",
    "                             .drop('duration').withColumn('duration', F.ceil('avg(duration)'))\\\n",
    "                             .drop('avg(duration)').cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Public transport edge list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@F.udf\n",
    "def hour(timestamp):\n",
    "    return timestamp[:2]\n",
    "\n",
    "# keep only reasonable hours\n",
    "min_day_hour, max_day_hour = 8, 20\n",
    "all_info = all_info.filter(hour(F.col('arrival_time')).cast('int').between(min_day_hour, max_day_hour))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Window\n",
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "\n",
    "def get_weekday_trips(trip_info):        \n",
    "    daily_trips = spark.createDataFrame([], StructType(trip_info.schema.fields + [StructField('weekday', StringType(), True)]))\n",
    "\n",
    "    weekdays = ['monday', 'tuesday', 'wednesday', 'thursday', 'friday']\n",
    "    for weekday in weekdays:\n",
    "        daily_trips = daily_trips.union(trip_info.filter(weekday + ' = \"1\"').withColumn('weekday', F.lit(weekday)))\n",
    "    \n",
    "    return daily_trips\n",
    "\n",
    "@F.udf\n",
    "def minutes(timestamp):\n",
    "    return int(timestamp[:2]) * 60 + int(timestamp[3:5])\n",
    "\n",
    "# needs columns :{trip_id, stop_sequence, arrival_time, departure_time}\n",
    "def get_edges(trip_info):\n",
    "    \n",
    "    trips_per_weekday = get_weekday_trips(trip_info)\n",
    "\n",
    "    window = Window.partitionBy(['trip_id', 'weekday']).orderBy(F.col('stop_sequence').cast('int'))\n",
    "\n",
    "    edges = trips_per_weekday.withColumn('arrival_time_minutes', minutes(F.col('arrival_time')).cast('int'))\n",
    "    edges = edges.withColumn('departure_time_minutes', minutes(F.col('departure_time')).cast('int'))\n",
    "    \n",
    "    edges = edges.withColumn(\"prev_departure_minutes\", F.lag(F.col('departure_time_minutes')).over(window))\n",
    "    edges = edges.withColumn(\"duration\", F.col('arrival_time_minutes') - F.col('prev_departure_minutes'))\n",
    "    \n",
    "    edges = edges.withColumn(\"start_vertex\", F.lag(F.col('stop_id')).over(window))\n",
    "    edges = edges.withColumnRenamed(\"stop_id\", \"end_vertex\")\n",
    "    edges = edges.withColumnRenamed('prev_departure_minutes', 'start_time')\n",
    "    \n",
    "    edges = edges.filter(\"prev_departure_minutes is not null\") # removes start of trip\n",
    "    \n",
    "    edges = edges.drop_duplicates(['start_vertex', 'end_vertex', 'start_time', 'route_id', 'weekday'])\n",
    "    \n",
    "    return edges.select('start_vertex', 'end_vertex', 'start_time', 'duration', 'route_id', 'weekday')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transport_edges = get_edges(all_info).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transport_edges.groupby(['start_vertex', 'end_vertex', 'start_time', 'route_id', 'weekday']).count().where('count > 1').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reachable stops from Zurich HB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_edges = transport_edges.select('start_vertex', 'end_vertex')\\\n",
    "                             .union(walking_edges.select('start_vertex', 'end_vertex'))\\\n",
    "                             .distinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_unreachable(vertices, edges):\n",
    "    v = vertices.withColumnRenamed('stop_id', 'id')\n",
    "    e = edges.withColumnRenamed('start_vertex', 'src').withColumnRenamed('end_vertex', 'dst')\n",
    "    \n",
    "    g = GraphFrame(v, e)\n",
    "    \n",
    "    cc = g.connectedComponents(algorithm='graphx')\n",
    "    \n",
    "    zurich_component = cc.filter(\"id == '8503000'\").select('component').collect()[0][0]\n",
    "    \n",
    "    return cc[cc.component == zurich_component].select('id').withColumnRenamed('id', 'stop_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reachable_stops = filter_unreachable(stops.select('stop_id').distinct(), graph_edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reachable_edges = graph_edges.join(reachable_stops, graph_edges.start_vertex == reachable_stops.stop_id)\\\n",
    "                             .select('start_vertex', 'end_vertex')\\\n",
    "                             .join(reachable_stops, graph_edges.end_vertex == reachable_stops.stop_id)\\\n",
    "                             .select('start_vertex', 'end_vertex')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_transport_edges = transport_edges.join(reachable_edges, ['start_vertex', 'end_vertex'], \"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_walking_edges = walking_edges.join(reachable_edges, ['start_vertex', 'end_vertex'], \"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_reachable_stops = stops.join(reachable_stops, \"stop_id\", 'inner')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_reachable_stops.write.mode('overwrite').parquet('/user/%s/final/parquet/reachable_stops' % username)\n",
    "final_transport_edges.write.mode('overwrite').parquet('/user/%s/final/parquet/transport_edges' % username)\n",
    "final_walking_edges.write.mode('overwrite').parquet('/user/%s/final/parquet/walking_edges' % username)\n",
    "routes.write.mode('overwrite').parquet('/user/%s/final/parquet/routes' % username)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%local\n",
    "from hdfs3 import HDFileSystem\n",
    "hdfs = HDFileSystem(user='ebouille') # impersonate ebouille to read the file\n",
    "hdfs.ls('/user/%s/final/parquet' % username)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
