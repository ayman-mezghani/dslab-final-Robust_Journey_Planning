{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Spark environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%local\n",
    "import os\n",
    "import json\n",
    "\n",
    "username = os.environ['JUPYTERHUB_USER']\n",
    "\n",
    "configuration = dict(\n",
    "    name = \"%s-final-schedule\" % username,\n",
    "    executorMemory = \"4G\",\n",
    "    executorCores = 4,\n",
    "    numExecutors = 10,\n",
    "    driverMemory = \"4G\",\n",
    "    conf = {\n",
    "#         \"spark.pyspark.python\": \"/opt/anaconda3/bin/python3\", # Use python3\n",
    "        \"spark.jars.repositories\": \"https://repos.spark-packages.org\",\n",
    "        \"spark.jars.packages\": \"graphframes:graphframes:0.7.0-spark2.3-s_2.11\"\n",
    "    }\n",
    ")\n",
    "\n",
    "# set the application name as \"<your_gaspar_id>-homework3\"\n",
    "get_ipython().run_cell_magic('configure', line=\"-f\", cell=json.dumps(configuration))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%send_to_spark -i username -t str -n username"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('We are using Spark %s' % spark.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.addPyFile('graphframes_graphframes-0.7.0-spark2.3-s_2.11.jar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from graphframes import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data into spark dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stops = spark.read.orc('/data/sbb/orc/geostops')\n",
    "stop_times = spark.read.csv(\"/data/sbb/csv/timetable/stop_times/2019/05/07/stop_times.csv\", header=True).drop('pickup_type', 'drop_off_type')\n",
    "routes = spark.read.csv('/data/sbb/csv/timetable/routes/2019/05/07/routes.csv', header=True )\n",
    "calendar = spark.read.csv('/data/sbb/csv/timetable/calendar/2019/05/07/calendar.csv', header=True).drop('start_date','end_date')\n",
    "trips = spark.read.csv('/data/sbb/csv/timetable/trips/2019/05/07/trips.csv', header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter out stops out of the 15km radius from ZÃ¼rich HB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.functions import acos, asin, cos, sin, lit, toRadians, sqrt\n",
    "\n",
    "def haversine(theta):\n",
    "    return (lit(1) - cos(theta)) / lit(2)\n",
    "\n",
    "def haversine_dist(latitude_x, longitude_x, latitude_y, longitude_y):\n",
    "    latitude_x, longitude_x, latitude_y, longitude_y = toRadians(latitude_x), toRadians(longitude_x),\\\n",
    "                                                       toRadians(latitude_y), toRadians(longitude_y)\n",
    "    h = haversine(latitude_x - latitude_y) + cos(latitude_x) * cos(latitude_y) * haversine(longitude_x - longitude_y)\n",
    "    earth_radius = 6371.0\n",
    "    return acos(lit(1) - lit(2) * h) * earth_radius"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leave only stops in 15 km radius\n",
    "zurich_HB_lat, zurich_HB_lon = 47.378177, 8.540192\n",
    "stops = stops.withColumn('distance_zurich_HB', haversine_dist(lit(zurich_HB_lat), lit(zurich_HB_lon), stops.stop_lat, stops.stop_lon))\n",
    "stops = stops.filter(stops.distance_zurich_HB <= 15)\n",
    "all_info = stops.join(stop_times.join(trips.join(calendar, on='service_id'), on='trip_id'), on='stop_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute edge lists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distances between stops and walking edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stops_start = stops.withColumnRenamed('stop_id', 'start_vertex')\\\n",
    "                   .withColumnRenamed('stop_lat', 'stop_lat_start')\\\n",
    "                   .withColumnRenamed('stop_lon', 'stop_lon_start')\\\n",
    "\n",
    "stops_end = stops.withColumnRenamed('stop_id', 'end_vertex')\\\n",
    "                 .withColumnRenamed('stop_lat', 'stop_lat_end')\\\n",
    "                 .withColumnRenamed('stop_lon', 'stop_lon_end')\\\n",
    "              \n",
    "\n",
    "all_distances = stops_start.crossJoin(stops_end).withColumn('distance', haversine_dist(F.col('stop_lat_start'), F.col('stop_lon_start'),\n",
    "                                                                                       F.col('stop_lat_end'), F.col('stop_lon_end')))\n",
    "\n",
    "#                .withColumn('trip_id', F.lit('-1'))\\\n",
    "#                .withColumn('start_time', F.lit(-1))\\\n",
    "\n",
    "walking_speed = 0.05\n",
    "walking_edges = all_distances.filter((F.col('distance') <= 0.5) & (F.col('start_vertex') != F.col('end_vertex')))\\\n",
    "               .withColumn('duration', F.ceil(F.col('distance') / walking_speed))\\\n",
    "               .withColumn('route_id', F.lit('walking'))\\\n",
    "               .select('start_vertex', 'end_vertex', 'duration', 'route_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Public transport edge list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@F.udf\n",
    "def hour(timestamp):\n",
    "    return timestamp[:2]\n",
    "\n",
    "# keep only reasonable hours\n",
    "min_day_hour, max_day_hour = 8, 20\n",
    "all_info = all_info.filter(hour(F.col('arrival_time')).cast('int').between(min_day_hour, max_day_hour))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Window\n",
    "\n",
    "@F.udf\n",
    "def minutes(timestamp):\n",
    "    return int(timestamp[:2]) * 60 + int(timestamp[3:5])\n",
    "\n",
    "# needs columns :{trip_id, stop_sequence, arrival_time, departure_time}\n",
    "def get_edges(trip_info):\n",
    "    \n",
    "    window = Window.partitionBy('trip_id').orderBy(F.col('stop_sequence').cast('int'))\n",
    "\n",
    "    edges = trip_info.withColumn('arrival_time_minutes', minutes(F.col('arrival_time')).cast('int'))\n",
    "    edges = edges.withColumn('departure_time_minutes', minutes(F.col('departure_time')).cast('int'))\n",
    "    \n",
    "    edges = edges.withColumn(\"prev_departure_minutes\", F.lag(F.col('departure_time_minutes')).over(window))\n",
    "    edges = edges.withColumn(\"duration\", F.col('arrival_time_minutes') - F.col('prev_departure_minutes'))\n",
    "    \n",
    "    edges = edges.withColumn(\"start_vertex\", F.lag(F.col('stop_id')).over(window))\n",
    "    edges = edges.withColumnRenamed(\"stop_id\", \"end_vertex\")\n",
    "    edges = edges.withColumnRenamed('prev_departure_minutes', 'start_time')\n",
    "    \n",
    "    edges = edges.filter(\"prev_departure_minutes is not null\") # removes start of trip\n",
    "    \n",
    "    return edges.select('start_vertex', 'end_vertex', 'start_time', 'duration', 'route_id').drop_duplicates(['start_vertex','end_vertex', 'start_time', 'route_id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reachable edges from Zurich HB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_edges = transport_edges.select('start_vertex', 'end_vertex')\\\n",
    "                             .union(walking_edges.select('start_vertex', 'end_vertex'))\\\n",
    "                             .distinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_unreachable(vertices, edges):\n",
    "    v = vertices.withColumnRenamed('stop_id', 'id')\n",
    "    e = edges.withColumnRenamed('start_vertex', 'src').withColumnRenamed('end_vertex', 'dst')\n",
    "    \n",
    "    g = GraphFrame(v, e)\n",
    "    \n",
    "    cc = g.connectedComponents(algorithm='graphx')\n",
    "    \n",
    "    zurich_component = cc.filter(\"id == '8503000'\").select('component').collect()[0][0]\n",
    "    \n",
    "    return cc[cc.component == zurich_component].select('id').withColumnRenamed('id', 'stop_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reachable_stops = filter_unreachable(stops.select('stop_id').distinct(), graph_edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reachable_edges = graph_edges.join(reachable_stops, graph_edges.start_vertex == reachable_stops.stop_id)\\\n",
    "                             .select('start_vertex', 'end_vertex')\\\n",
    "                             .join(reachable_stops, graph_edges.end_vertex == reachable_stops.stop_id)\\\n",
    "                             .select('start_vertex', 'end_vertex')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reachable_edges.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reachable_stops.write.parquet('/user/%s/final/parquet/reachable_stops' % username)\n",
    "reachable_edges.write.parquet('/user/%s/final/parquet/reachable_edges' % username)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%local\n",
    "from hdfs3 import HDFileSystem\n",
    "hdfs = HDFileSystem(user='ebouille') # impersonate ebouille to read the file\n",
    "hdfs.ls('/user/%s/final/parquet' % username)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shortest Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inward_walking_edges = walking_edges.groupby('end_vertex').agg({'start_vertex': 'collect_set'})\\\n",
    "                                                              .withColumnRenamed('collect_set(start_vertex)', 'start_vertices')\\\n",
    "                                                              .toPandas().set_index('end_vertex').to_dict()['start_vertices']\n",
    "walking_edge_duration = walking_edges.toPandas()\n",
    "walking_edge_duration['key'] = walking_edge_duration.apply(lambda x: (x['start_vertex'], x['end_vertex']), axis=1)\n",
    "walking_edge_duration = walking_edge_duration.set_index('key').to_dict()['duration']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges = get_edges(all_info.filter(F.col('monday') == '1')).sort(F.col('start_time').desc())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function assumes a (spark) dataframe with columns 'start_vertex', 'end_vertex', 'start_time', 'duration', 'route_id' \n",
    "# edges is a dataframe that represents the edges of the graph, \n",
    "# edges should be sorted by order of descending starting time\n",
    "# inward_walking_edges is a map route_id -> list(route_id), representing the nodes from which we can walk to a given node\n",
    "# walking_edge_duration is a map (route_id, route_id) -> float, representing the duration of the walk\n",
    "# end_time should be in the format 'HH:MM'\n",
    "def latest_departure_paths(edges, end_stop, end_time, inward_walking_edges, walking_edge_duration):\n",
    "    \n",
    "    # This function computes whether edge (u, v, t, dur, route_id) can be taken\n",
    "    # returns update to node u if possible, and None otherwise\n",
    "    def edge_valid(u, v, time, dur, route_id):\n",
    "        \n",
    "        cur_best = next_transfer.get(u)\n",
    "        next_edge = next_transfer.get(v)\n",
    "        \n",
    "        if next_edge: # needs to be defined\n",
    "            time_v, next_v, route_id_v, dur_v = next_edge\n",
    "            \n",
    "            if route_id == 'walking':\n",
    "                if  route_id_v == 'walking' and dur_v + dur <= max_walking_time:\n",
    "                    time = time_v - dur - transfer_time\n",
    "                    if (not cur_best) or time > cur_best[0]:\n",
    "                        return (time, next_v, route_id, dur + dur_v)\n",
    "                elif route_id_v != 'walking':\n",
    "                    time = time_v - dur - transfer_time\n",
    "                    if (not cur_best) or time > cur_best[0]:\n",
    "                        return (time, v, route_id, dur)\n",
    "                      \n",
    "            \n",
    "            elif (route_id == route_id_v or v == end_stop) and time + duration <= time_v: # same route or last_stop\n",
    "                if (not cur_best) or time > cur_best[0]: # can we improve\n",
    "                    return (time, next_v, route_id, dur + dur_v)\n",
    "            \n",
    "            \n",
    "            elif time + duration + transfer_time <= time_v: # transfer at node_v\n",
    "                if (not cur_best) or time > cur_best[0]:\n",
    "                    return (time, v, route_id, dur) \n",
    "        \n",
    "        # In any other case no update can be made\n",
    "        return None\n",
    "                \n",
    "    def time_to_minutes(timestamp):\n",
    "        return int(timestamp[:2]) * 60 + int(timestamp[3:5])\n",
    "    \n",
    "    transfer_time = 2 # at least 2 minutes to transfer\n",
    "    max_walking_time = 10\n",
    "\n",
    "    next_transfer = {} # stores for each node u the node v \n",
    "                       # where the next transfer happens format (departure_time, v, route_id to v, duration)\n",
    "    end_time = time_to_minutes(end_time)\n",
    "    start_time = end_time - 120 # look only at edges departing at most 2 hours before end_time\n",
    "    next_transfer[end_stop] = (end_time, end_stop, None, 0)\n",
    "    \n",
    "    edges = edges.filter(F.col('start_time').between(start_time, end_time)).toPandas()\n",
    "\n",
    "    for _, row in edges.iterrows():\n",
    "        \n",
    "        start_vertex, end_vertex, start, duration, route_id = row.start_vertex, row.end_vertex, row.start_time, row.duration, row.route_id\n",
    "        \n",
    "        update = edge_valid(start_vertex, end_vertex, start, duration, route_id)\n",
    "        if update:\n",
    "            next_transfer[start_vertex] = update\n",
    "                    \n",
    "            w_start_vertex = start_vertex\n",
    "            for w_end_vertex in inward_walking_edges.get(w_start_vertex, []):\n",
    "                duration = walking_edge_duration[(w_start_vertex, w_end_vertex)]\n",
    "                w_start = start \n",
    "                update = edge_valid(w_start_vertex, w_end_vertex, w_start, duration, 'walking')\n",
    "                if update:\n",
    "                     next_transfer[w_start_vertex] = update\n",
    "                    \n",
    "        \n",
    "    return next_transfer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "next_transfer = latest_departure_paths(edges, '8591221', '20:00', inward_walking_edges, walking_edge_duration)\n",
    "end = time.time() \n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minutes_to_timestamp(minutes):\n",
    "    return str(minutes // 60) + ':' + str(minutes % 60).zfill(2)\n",
    "    \n",
    "def reconstruct_route(next_transfer, start_stop):\n",
    "    time, next_stop, route_id, duration = next_transfer[start_stop]\n",
    "    journey = ''\n",
    "    while next_stop != start_stop:\n",
    "        journey += \"\"\"  \\n\n",
    "                        %s : %s \n",
    "                        | \n",
    "                        | %s (%s min) \n",
    "                        | \n",
    "                        %s : %s \"\"\" % (minutes_to_timestamp(time),\\\n",
    "                                       stop_id_to_stop[start_stop],\\\n",
    "                                       route_id_to_route_name[route_id],\\\n",
    "                                       duration, minutes_to_timestamp(time + duration), stop_id_to_stop[next_stop])\n",
    "        start_stop = next_stop\n",
    "        time, next_stop, route_id, duration = next_transfer[next_stop]\n",
    "        \n",
    "    return journey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "route_id_to_route_name = routes.select('route_id', 'route_desc', 'route_short_name').toPandas()\n",
    "route_id_to_route_name['route_name'] = route_id_to_route_name['route_desc'] + ' ' + route_id_to_route_name['route_short_name']\n",
    "route_id_to_route_name = route_id_to_route_name.set_index('route_id').to_dict()['route_name']\n",
    "route_id_to_route_name['walking'] = 'walk'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_id_to_stop = stops.select('stop_id', 'stop_name').toPandas().set_index('stop_id').to_dict()['stop_name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minutes_to_timestamp(minutes):\n",
    "    return str(minutes // 60) + ':' + str(minutes % 60).zfill(2)\n",
    "    \n",
    "def reconstruct_route(next_transfer, start_stop):\n",
    "    time, next_stop, route_id, duration = next_transfer[start_stop]\n",
    "    journey = ''\n",
    "    while next_stop != start_stop:\n",
    "        journey += \"\"\"  \\n\n",
    "                        %s : %s \n",
    "                        | \n",
    "                        | %s (%s min) \n",
    "                        | \n",
    "                        %s : %s \"\"\" % (minutes_to_timestamp(time),\\\n",
    "                                       stop_id_to_stop[start_stop],\\\n",
    "                                       route_id_to_route_name[route_id],\\\n",
    "                                       duration, minutes_to_timestamp(time + duration), stop_id_to_stop[next_stop])\n",
    "        start_stop = next_stop\n",
    "        time, next_stop, route_id, duration = next_transfer[next_stop]\n",
    "        \n",
    "    return journey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(reconstruct_route(next_transfer, '8590883')) # almost same as sbb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(reconstruct_route(next_transfer, '8591353')) # almost same as sbb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(reconstruct_route(next_transfer, '8590788')) # not in sbb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(reconstruct_route(next_transfer, '8591368')) # Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(reconstruct_route(next_transfer, next_transfer.keys()[100])) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
