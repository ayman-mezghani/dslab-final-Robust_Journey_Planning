{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Spark environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%local\n",
    "import os\n",
    "import json\n",
    "\n",
    "username = os.environ['JUPYTERHUB_USER']\n",
    "\n",
    "configuration = dict(\n",
    "    name = \"%s-final-schedule\" % username,\n",
    "    executorMemory = \"4G\",\n",
    "    executorCores = 4,\n",
    "    numExecutors = 10,\n",
    "    driverMemory = \"4G\",\n",
    "    conf = {\n",
    "        # \"spark.pyspark.python\": \"/opt/anaconda3/bin/python3\", # Use python3\n",
    "        \"spark.jars.repositories\": \"https://repos.spark-packages.org\",\n",
    "        \"spark.jars.packages\": \"graphframes:graphframes:0.7.0-spark2.3-s_2.11\"\n",
    "    }\n",
    ")\n",
    "\n",
    "# set the application name as \"<your_gaspar_id>-homework3\"\n",
    "get_ipython().run_cell_magic('configure', line=\"-f\", cell=json.dumps(configuration))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%send_to_spark -i username -t str -n username"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('We are using Spark %s' % spark.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.addPyFile('graphframes_graphframes-0.7.0-spark2.3-s_2.11.jar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from graphframes import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data into spark dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stops = spark.read.orc('/data/sbb/orc/geostops')\n",
    "stop_times = spark.read.csv(\"/data/sbb/csv/timetable/stop_times/2019/05/07/stop_times.csv\", header=True).drop('pickup_type', 'drop_off_type')\n",
    "routes = spark.read.csv('/data/sbb/csv/timetable/routes/2019/05/07/routes.csv', header=True )\n",
    "calendar = spark.read.csv('/data/sbb/csv/timetable/calendar/2019/05/07/calendar.csv', header=True).drop('start_date','end_date')\n",
    "trips = spark.read.csv('/data/sbb/csv/timetable/trips/2019/05/07/trips.csv', header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter out stops out of the 15km radius from ZÃ¼rich HB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.functions import acos, asin, cos, sin, lit, toRadians, sqrt\n",
    "\n",
    "def haversine(theta):\n",
    "    return (lit(1) - cos(theta)) / lit(2)\n",
    "\n",
    "def haversine_dist(latitude_x, longitude_x, latitude_y, longitude_y):\n",
    "    latitude_x, longitude_x, latitude_y, longitude_y = toRadians(latitude_x), toRadians(longitude_x),\\\n",
    "                                                       toRadians(latitude_y), toRadians(longitude_y)\n",
    "    h = haversine(latitude_x - latitude_y) + cos(latitude_x) * cos(latitude_y) * haversine(longitude_x - longitude_y)\n",
    "    earth_radius = 6371.0\n",
    "    return acos(lit(1) - lit(2) * h) * earth_radius"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leave only stops in 15 km radius\n",
    "zurich_HB_lat, zurich_HB_lon = 47.378177, 8.540192\n",
    "stops = stops.withColumn('distance_zurich_HB', haversine_dist(lit(zurich_HB_lat), lit(zurich_HB_lon), stops.stop_lat, stops.stop_lon))\n",
    "stops = stops.filter(stops.distance_zurich_HB <= 15)\n",
    "all_info = stops.join(stop_times.join(trips.join(calendar, on='service_id'), on='trip_id'), on='stop_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute adjacency list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stops_start = stops.withColumnRenamed('stop_id', 'start_vertex')\\\n",
    "                   .withColumnRenamed('stop_lat', 'stop_lat_start')\\\n",
    "                   .withColumnRenamed('stop_lon', 'stop_lon_start')\\\n",
    "              \n",
    "stops_end = stops.withColumnRenamed('stop_id', 'end_vertex')\\\n",
    "                 .withColumnRenamed('stop_lat', 'stop_lat_end')\\\n",
    "                 .withColumnRenamed('stop_lon', 'stop_lon_end')\\\n",
    "              \n",
    "\n",
    "all_distances = stops_start.crossJoin(stops_end).withColumn('distance', haversine_dist(F.col('stop_lat_start'), F.col('stop_lon_start'),\n",
    "                                                                                       F.col('stop_lat_end'), F.col('stop_lon_end')))\n",
    "\n",
    "walking_speed = 0.05\n",
    "walking_edges = all_distances.filter((F.col('distance') <= 0.5) & (F.col('start_vertex') != F.col('end_vertex')))\\\n",
    "               .withColumn('duration', F.col('distance') / walking_speed)\\\n",
    "               .withColumn('start_time', F.lit(-1))\\\n",
    "               .withColumn('trip_id', F.lit('-1'))\\\n",
    "               .select('start_vertex', 'end_vertex', 'start_time', 'duration', 'trip_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@F.udf\n",
    "def hour(timestamp):\n",
    "    return timestamp[:2]\n",
    "\n",
    "# keep only reasonable hours\n",
    "min_day_hour, max_day_hour = 6, 22\n",
    "all_info = all_info.filter(hour(F.col('arrival_time')).cast('int').between(min_day_hour, max_day_hour))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Window\n",
    "\n",
    "@F.udf\n",
    "def minutes(timestamp):\n",
    "    return int(timestamp[:2]) * 60 + int(timestamp[3:5])\n",
    "\n",
    "# needs columns :{trip_id, stop_sequence, arrival_time, departure_time}\n",
    "def get_edges(trip_info):\n",
    "    \n",
    "    window = Window.partitionBy('trip_id').orderBy(F.col('stop_sequence').cast('int'))\n",
    "\n",
    "    edges = trip_info.withColumn('arrival_time_minutes', minutes(F.col('arrival_time')).cast('int'))\n",
    "    edges = edges.withColumn('departure_time_minutes', minutes(F.col('departure_time')).cast('int'))\n",
    "    \n",
    "    edges = edges.withColumn(\"prev_departure_minutes\", F.lag(F.col('departure_time_minutes')).over(window))\n",
    "    edges = edges.withColumn(\"duration\", F.col('arrival_time_minutes') - F.col('prev_departure_minutes'))\n",
    "    \n",
    "    edges = edges.withColumn(\"start_vertex\", F.lag(F.col('stop_id')).over(window))\n",
    "    edges = edges.withColumnRenamed(\"stop_id\", \"end_vertex\")\n",
    "    edges = edges.withColumnRenamed('prev_departure_minutes', 'start_time')\n",
    "    \n",
    "    edges = edges.filter(\"prev_departure_minutes is not null\") # removes start of trip\n",
    "    \n",
    "    return edges.select('start_vertex', 'end_vertex', 'start_time', 'duration', 'trip_id', 'route_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_edges = get_edges(all_info).select('start_vertex', 'end_vertex')\\\n",
    "                                 .union(walking_edges.select('start_vertex', 'end_vertex'))\\\n",
    "                                 .withColumnRenamed('start_vertex', 'src')\\\n",
    "                                 .withColumnRenamed('end_vertex', 'dst')\\\n",
    "                                 .distinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_unreachable(vertices, edges):\n",
    "    v = vertices.withColumnRenamed('stop_id', 'id')\n",
    "    e = edges.withColumnRenamed('start_vertex', 'src').withColumnRenamed('end_vertex', 'dst')\n",
    "    \n",
    "    g = GraphFrame(v, e)\n",
    "    \n",
    "    cc = g.connectedComponents(algorithm='graphx')\n",
    "    \n",
    "    zurich_component = cc.filter(\"id == '8503000'\").select('component').collect()[0][0]\n",
    "    \n",
    "    return cc[cc.component == zurich_component].select('id').withColumnRenamed('id', 'stop_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reachable_stops = filter_unreachable(stops.select('stop_id').distinct(), graph_edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reachable_stops.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
