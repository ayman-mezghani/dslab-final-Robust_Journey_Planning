{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Spark environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%local\n",
    "import os\n",
    "import json\n",
    "\n",
    "username = os.environ['JUPYTERHUB_USER']\n",
    "\n",
    "configuration = dict(\n",
    "    name = \"%s-final-schedule\" % username,\n",
    "    executorMemory = \"4G\",\n",
    "    executorCores = 4,\n",
    "    numExecutors = 10,\n",
    "    driverMemory = \"4G\",\n",
    "    conf = {\n",
    "        # \"spark.pyspark.python\": \"/opt/anaconda3/bin/python3\", # Use python3\n",
    "        \"spark.jars.repositories\": \"https://repos.spark-packages.org\",\n",
    "        \"spark.jars.packages\": \"graphframes:graphframes:0.7.0-spark2.3-s_2.11\"\n",
    "    }\n",
    ")\n",
    "\n",
    "# set the application name as \"<your_gaspar_id>-homework3\"\n",
    "get_ipython().run_cell_magic('configure', line=\"-f\", cell=json.dumps(configuration))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%send_to_spark -i username -t str -n username"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('We are using Spark %s' % spark.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.addPyFile('graphframes_graphframes-0.7.0-spark2.3-s_2.11.jar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from graphframes import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data into spark dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stops = spark.read.orc('/data/sbb/orc/geostops')\n",
    "stop_times = spark.read.csv(\"/data/sbb/csv/timetable/stop_times/2019/05/07/stop_times.csv\", header=True).drop('pickup_type', 'drop_off_type')\n",
    "routes = spark.read.csv('/data/sbb/csv/timetable/routes/2019/05/07/routes.csv', header=True )\n",
    "calendar = spark.read.csv('/data/sbb/csv/timetable/calendar/2019/05/07/calendar.csv', header=True).drop('start_date','end_date')\n",
    "trips = spark.read.csv('/data/sbb/csv/timetable/trips/2019/05/07/trips.csv', header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter out stops out of the 15km radius from ZÃ¼rich HB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.functions import acos, asin, cos, sin, lit, toRadians, sqrt\n",
    "from pyspark.sql import Window\n",
    "\n",
    "\n",
    "def haversine(theta):\n",
    "    return (lit(1) - cos(theta)) / lit(2)\n",
    "\n",
    "def haversine_dist(latitude_x, longitude_x, latitude_y, longitude_y):\n",
    "    latitude_x, longitude_x, latitude_y, longitude_y = toRadians(latitude_x), toRadians(longitude_x),\\\n",
    "                                                       toRadians(latitude_y), toRadians(longitude_y)\n",
    "    h = haversine(latitude_x - latitude_y) + cos(latitude_x) * cos(latitude_y) * haversine(longitude_x - longitude_y)\n",
    "    earth_radius = 6371.0\n",
    "    return acos(lit(1) - lit(2) * h) * earth_radius"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leave only stops in 15 km radius\n",
    "zurich_HB_lat, zurich_HB_lon = 47.378177, 8.540192\n",
    "stops = stops.withColumn('distance_zurich_HB', haversine_dist(lit(zurich_HB_lat), lit(zurich_HB_lon), stops.stop_lat, stops.stop_lon))\n",
    "stops = stops.filter(stops.distance_zurich_HB <= 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove duplicate trips on each day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_trip_times = trips.join(stop_times, 'trip_id').join(calendar, 'service_id').cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_trip_times.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add column start time of service for a trip. Then each trip will have a starting time. Sort by length of `ord_stops` (largest on top), drop duplicates on route_id, direction, start_time. This will keep most generic trip on that route starting at a given time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@F.udf\n",
    "def sort_array(array):\n",
    "    return sorted(array)\n",
    "\n",
    "def get_daily_trips(df, day):\n",
    "    day_trips = df.filter(day + ' = \"1\"')\n",
    "\n",
    "    groupby_cols = ['trip_id', 'route_id', 'direction_id', 'trip_short_name', 'service_id']\n",
    "    res = day_trips.withColumn(\"ord_stop\", F.concat(F.col('stop_sequence'), F.lit(';'), F.col('stop_id')))\\\n",
    "                   .groupby(groupby_cols)\\\n",
    "                   .agg({x : 'collect_list' for x in day_trips.columns + ['ord_stop'] if x not in groupby_cols})\\\n",
    "                   .withColumn('departure_times', sort_array(F.col('collect_list(departure_time)')))\\\n",
    "                   .withColumn('arrival_times', sort_array(F.col('collect_list(arrival_time)')))\\\n",
    "                   .withColumn('ord_stops', sort_array(F.col('collect_list(ord_stop)')))\n",
    "                \n",
    "    cc = ['route_id', 'departure_times', 'arrival_times', 'ord_stops']\n",
    "    return res.drop_duplicates(cc).select('trip_id').withColumn('weekday', F.lit(day))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType,StructField, StringType\n",
    "\n",
    "daily_trips_schema = StructType([\n",
    "  StructField('trip_id', StringType(), True),\n",
    "  StructField('weekday', StringType(), True)\n",
    "  ])\n",
    "daily_trips = spark.createDataFrame([], daily_trips_schema)\n",
    "\n",
    "weekdays = ['monday', 'tuesday', 'wednesday', 'thursday', 'friday']\n",
    "\n",
    "for weekday in weekdays:\n",
    "    daily_trips = daily_trips.union(get_daily_trips(all_trip_times, weekday))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_trips.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute edge lists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distances between stops and walking edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stops_start = stops.withColumnRenamed('stop_id', 'start_vertex')\\\n",
    "                   .withColumnRenamed('stop_lat', 'stop_lat_start')\\\n",
    "                   .withColumnRenamed('stop_lon', 'stop_lon_start')\\\n",
    "              \n",
    "stops_end = stops.withColumnRenamed('stop_id', 'end_vertex')\\\n",
    "                 .withColumnRenamed('stop_lat', 'stop_lat_end')\\\n",
    "                 .withColumnRenamed('stop_lon', 'stop_lon_end')\\\n",
    "              \n",
    "\n",
    "all_distances = stops_start.crossJoin(stops_end).withColumn('distance', haversine_dist(F.col('stop_lat_start'), F.col('stop_lon_start'),\n",
    "                                                                                       F.col('stop_lat_end'), F.col('stop_lon_end')))\n",
    "\n",
    "walking_speed = 0.05\n",
    "walking_edges = all_distances.filter((F.col('distance') <= 0.5) & (F.col('start_vertex') != F.col('end_vertex')))\\\n",
    "               .withColumn('duration', F.col('distance') / walking_speed)\\\n",
    "               .withColumn('start_time', F.lit(-1))\\\n",
    "               .withColumn('trip_id', F.lit('-1'))\\\n",
    "               .withColumn('route_id', F.lit('walking'))\\\n",
    "               .select('start_vertex', 'end_vertex', 'start_time', 'duration', 'trip_id', 'route_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Public transport edge list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@F.udf\n",
    "def hour(timestamp):\n",
    "    return timestamp[:2]\n",
    "\n",
    "# keep only reasonable hours\n",
    "min_day_hour, max_day_hour = 6, 22\n",
    "all_info = all_info.filter(hour(F.col('arrival_time')).cast('int').between(min_day_hour, max_day_hour))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@F.udf\n",
    "def minutes(timestamp):\n",
    "    return int(timestamp[:2]) * 60 + int(timestamp[3:5])\n",
    "\n",
    "# needs columns :{trip_id, stop_sequence, arrival_time, departure_time}\n",
    "def get_edges(trip_info):\n",
    "    \n",
    "    window = Window.partitionBy('trip_id').orderBy(F.col('stop_sequence').cast('int'))\n",
    "\n",
    "    edges = trip_info.withColumn('arrival_time_minutes', minutes(F.col('arrival_time')).cast('int'))\n",
    "    edges = edges.withColumn('departure_time_minutes', minutes(F.col('departure_time')).cast('int'))\n",
    "    \n",
    "    edges = edges.withColumn(\"prev_departure_minutes\", F.lag(F.col('departure_time_minutes')).over(window))\n",
    "    edges = edges.withColumn(\"duration\", F.col('arrival_time_minutes') - F.col('prev_departure_minutes'))\n",
    "    \n",
    "    edges = edges.withColumn(\"start_vertex\", F.lag(F.col('stop_id')).over(window))\n",
    "    edges = edges.withColumnRenamed(\"stop_id\", \"end_vertex\")\n",
    "    edges = edges.withColumnRenamed('prev_departure_minutes', 'start_time')\n",
    "    \n",
    "    edges = edges.filter(\"prev_departure_minutes is not null\") # removes start of trip\n",
    "    \n",
    "    return edges.select('start_vertex', 'end_vertex', 'start_time', 'duration', 'trip_id', 'route_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transport_edges = get_edges(all_info).join(daily_trips, 'trip_id').cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transport_edges.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transport_edges.filter('start_vertex = \"8503064\" and end_vertex = \"8503065:0:1\" and start_time = \"641\"').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transport_edges.groupBy(transport_edges.columns[1:]).count().filter('count > 1').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import collect_set\n",
    "s = transport_edges.groupby(['start_vertex', 'end_vertex', 'start_time', 'route_id']).agg(collect_set('duration'), collect_set('trip_id'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss = s.withColumn(\"ss\", F.size(F.col('collect_set(duration)')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reachable edges from Zurich HB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_edges = transport_edges.select('start_vertex', 'end_vertex')\\\n",
    "                             .union(walking_edges.select('start_vertex', 'end_vertex'))\\\n",
    "                             .distinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_unreachable(vertices, edges):\n",
    "    v = vertices.withColumnRenamed('stop_id', 'id')\n",
    "    e = edges.withColumnRenamed('start_vertex', 'src').withColumnRenamed('end_vertex', 'dst')\n",
    "    \n",
    "    g = GraphFrame(v, e)\n",
    "    \n",
    "    cc = g.connectedComponents(algorithm='graphx')\n",
    "    \n",
    "    zurich_component = cc.filter(\"id == '8503000'\").select('component').collect()[0][0]\n",
    "    \n",
    "    return cc[cc.component == zurich_component].select('id').withColumnRenamed('id', 'stop_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reachable_stops = filter_unreachable(stops.select('stop_id').distinct(), graph_edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reachable_edges = graph_edges.join(reachable_stops, graph_edges.start_vertex == reachable_stops.stop_id)\\\n",
    "                             .select('start_vertex', 'end_vertex')\\\n",
    "                             .join(reachable_stops, graph_edges.end_vertex == reachable_stops.stop_id)\\\n",
    "                             .select('start_vertex', 'end_vertex')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reachable_edges.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reachable_stops.write.parquet('/user/%s/final/parquet/reachable_stops' % username)\n",
    "reachable_edges.write.parquet('/user/%s/final/parquet/reachable_edges' % username)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%local\n",
    "from hdfs3 import HDFileSystem\n",
    "hdfs = HDFileSystem(user='ebouille') # impersonate ebouille to read the file\n",
    "hdfs.ls('/user/%s/final/parquet' % username)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calendar.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
